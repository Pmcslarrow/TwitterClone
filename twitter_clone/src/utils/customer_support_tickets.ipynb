{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07fe898-0148-4dcb-9659-6a2a323866f6",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d985cae-619e-4f96-984e-4d9e34a5dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv(\"./customer_support_tickets.csv\")\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293feb7-0301-43c3-b2d3-03fb6f60e095",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31957763-0ca2-4f11-8b88-cb8306f72805",
   "metadata": {},
   "source": [
    "# Preprocessing Data\n",
    "\n",
    "### Selecting appropriate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d370f8a-1b27-4403-907d-ea54ab1a5e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "legacy_cols = [\"Ticket Subject\", \"Ticket Description\", \"Ticket Priority\", \"Ticket Type\"] \n",
    "df = df[legacy_cols] \n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c3818c-c76f-4f6d-9f05-4d504a72280f",
   "metadata": {},
   "source": [
    "### Removing the {product_purchased} with a NULL value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8712e1f-fe13-421a-b134-2193a3be3537",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticket Description\"] = df[\"Ticket Description\"].str.replace('{product_purchased}', '[NULL]')\n",
    "df[\"text\"] = df[\"Ticket Subject\"] + \" | \" + df[\"Ticket Description\"]\n",
    "df.drop(columns=[\"Ticket Subject\", \"Ticket Description\"], inplace=True)\n",
    "df[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaf11b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_lengths = df[\"text\"].str.len()\n",
    "average_length = text_lengths.mean()\n",
    "min_length = text_lengths.min()\n",
    "max_length = text_lengths.max()\n",
    "\n",
    "average_length, min_length, max_length\n",
    "\n",
    "#(np.float64(287.25162356830793), np.int64(149), np.int64(390)) max length should be 400"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bd56f-6642-457e-9a12-a9f98a00e971",
   "metadata": {},
   "source": [
    "### Tokenization \n",
    "\n",
    "I utilize one-hot encoding here because we want CrossEntropy loss to compare the distributions.\n",
    "\n",
    "array([[1., 0., 0., 0.],\n",
    "       [1., 0., 0., 0.],\n",
    "       [0., 0., 1., 0.],\n",
    "       ...,\n",
    "       [0., 1., 0., 0.],\n",
    "       [0., 0., 0., 1.],\n",
    "       [0., 1., 0., 0.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27376e9d-a79f-4d06-9cc9-87200b1468e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision scikit-learn transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a50636-4b81-4c58-a335-11d307df5d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6830887-7eac-4a92-87e1-0ea7d7058ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticket Priority\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba4ac54-6b73-4987-bb5f-927c8f346df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Ticket Type\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a3d400-8a94-4cda-b145-3f2809282acf",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "\n",
    "\n",
    "#### Tokenization / Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4977149a-fd29-4d4f-97d3-0bb4374a82d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping the labels to integers\n",
    "priority_map = {label: idx for idx, label in enumerate(df[\"Ticket Priority\"].unique())}\n",
    "type_map = {label: idx for idx, label in enumerate(df[\"Ticket Type\"].unique())}\n",
    "\n",
    "priority_map_idx_to_label = {idx: label for label, idx in priority_map.items()}\n",
    "type_map_idx_to_label = {idx: label for label, idx in type_map.items()}\n",
    "\n",
    "df[\"priority_label\"] = df[\"Ticket Priority\"].map(priority_map)\n",
    "df[\"type_label\"] = df[\"Ticket Type\"].map(type_map)\n",
    "\n",
    "# Tokenizing the text with BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tokenized = tokenizer(list(df[\"text\"]), padding=True, truncation=True, return_tensors=\"pt\", max_length=400) # input_ids, attention_mask, token_type_ids\n",
    "\n",
    "print(priority_map_idx_to_label)\n",
    "print(type_map_idx_to_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29103071",
   "metadata": {},
   "source": [
    "#### Pytorch Dataset Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa525655",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicketDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, priority_labels, type_labels):\n",
    "        self.encodings = encodings # tokenized\n",
    "        self.priority_labels = torch.tensor(priority_labels, dtype=torch.long)\n",
    "        self.type_labels = torch.tensor(type_labels, dtype=torch.long)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item[\"priority_labels\"] = self.priority_labels[idx]\n",
    "        item[\"type_labels\"] = self.type_labels[idx]\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.priority_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec5383d",
   "metadata": {},
   "source": [
    "#### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b9cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_encodings = tokenizer(list(train_df[\"text\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "val_encodings = tokenizer(list(val_df[\"text\"]), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "train_dataset = TicketDataset(train_encodings, train_df[\"priority_label\"].tolist(), train_df[\"type_label\"].tolist())\n",
    "val_dataset = TicketDataset(val_encodings, val_df[\"priority_label\"].tolist(), val_df[\"type_label\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e418e726",
   "metadata": {},
   "source": [
    "#### Printing a sample row "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e8ce61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample = train_dataset[0]\n",
    "\n",
    "print(sample.keys())\n",
    "\n",
    "print(\"Priority Label (y1):\", sample[\"priority_labels\"].item(), \"--> \", priority_map_idx_to_label[sample[\"priority_labels\"].item()])\n",
    "print(\"Type Label (y2):\", sample[\"type_labels\"].item(), \"--> \", type_map_idx_to_label[sample[\"type_labels\"].item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35d091d",
   "metadata": {},
   "source": [
    "#### Multi-headed BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c214c1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "import torch.nn as nn\n",
    "\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, hidden_size=768, num_priorities=4, num_types=5):  \n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.priority_head = nn.Linear(hidden_size, num_priorities)\n",
    "        self.type_head = nn.Linear(hidden_size, num_types)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(outputs.pooler_output)\n",
    "        return self.priority_head(pooled), self.type_head(pooled)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "{0: 'Critical', 1: 'Low', 2: 'High', 3: 'Medium'}\n",
    "{0: 'Technical issue', 1: 'Billing inquiry', 2: 'Cancellation request', 3: 'Product inquiry', 4: 'Refund request'}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1872bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71a4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MultiTaskBERT(num_priorities=4, num_types=5).to(device)  \n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e45822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for epoch in range(3):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        priority_labels = batch[\"priority_labels\"].to(device)\n",
    "        type_labels = batch[\"type_labels\"].to(device)\n",
    "\n",
    "        priority_logits, type_logits = model(input_ids, attention_mask)\n",
    "\n",
    "        # priority_logits and type_logits will have shape (batch_size, num_priorities)\n",
    "        # we will utilize CrossEntropyLoss for both tasks\n",
    "\n",
    "        loss1 = loss_fn(priority_logits, priority_labels)\n",
    "        loss2 = loss_fn(type_logits, type_labels)\n",
    "        loss = loss1 + loss2\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "twitter_clone.env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
